# test_set_completion.py
import sys
import os
from datetime import datetime, timedelta
from dotenv import load_dotenv

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

# –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

def create_test_set(level: str, set_name: str, word_count: int):
    """–°–æ–∑–¥–∞–µ—Ç —Ç–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä —Å–ª–æ–≤."""
    from pathlib import Path
    from config import LEVELS_DIR
    
    level_dir = Path(LEVELS_DIR) / level
    level_dir.mkdir(parents=True, exist_ok=True)
    
    set_file = level_dir / f"{set_name}.txt"
    words = []
    for i in range(1, word_count + 1):
        words.append(f"testword{i} - —Ç–µ—Å—Ç—Å–ª–æ–≤–æ{i}")
    
    with open(set_file, 'w', encoding='utf-8') as f:
        f.write('\n'.join(words))
    
    return words

def cleanup_test_user(chat_id: int):
    """–û—á–∏—â–∞–µ—Ç —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."""
    from database.db import db_manager
    from utils.helpers import daily_words_cache, previous_daily_words, reset_daily_words_cache
    
    try:
        with db_manager.transaction() as tx:
            tx.execute("DELETE FROM users WHERE chat_id = ?", (chat_id,))
            tx.execute("DELETE FROM dictionary WHERE chat_id = ?", (chat_id,))
            tx.execute("DELETE FROM learned_words WHERE chat_id = ?", (chat_id,))
        
        # –û—á–∏—â–∞–µ–º –∫—ç—à–∏
        reset_daily_words_cache(chat_id)
        if chat_id in previous_daily_words:
            del previous_daily_words[chat_id]
            
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏: {e}")

def setup_test_user(chat_id: int, level: str = "A1", words_per_day: int = 10):
    """–°–æ–∑–¥–∞–µ—Ç —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å –∑–∞–¥–∞–Ω–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏."""
    from database import crud
    from handlers.settings import user_set_selection
    
    # –û—á–∏—â–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –¥–∞–Ω–Ω—ã–µ
    cleanup_test_user(chat_id)
    
    # –°–æ–∑–¥–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    crud.add_user(chat_id)
    crud.update_user_level(chat_id, level)
    crud.update_user_words_per_day(chat_id, words_per_day)
    crud.update_user_notifications(chat_id, 3)  # 3 –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è
    
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä
    test_set = "TestSet"
    crud.update_user_chosen_set(chat_id, test_set)
    user_set_selection[chat_id] = test_set
    
    return test_set

def simulate_learning_words(chat_id: int, words_to_learn: list):
    """–°–∏–º—É–ª–∏—Ä—É–µ—Ç –∏–∑—É—á–µ–Ω–∏–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Å–ª–æ–≤."""
    from database import crud
    from utils.visual_helpers import extract_english
    
    today = datetime.now().strftime("%Y-%m-%d")
    
    for word in words_to_learn:
        english_part = extract_english(word)
        translation = word.split(" - ")[1] if " - " in word else f"–ø–µ—Ä–µ–≤–æ–¥_{english_part}"
        crud.add_learned_word(chat_id, english_part, translation, today)

def simulate_leftover_transition(chat_id: int, leftover_words: list):
    """–°–∏–º—É–ª–∏—Ä—É–µ—Ç –ø–µ—Ä–µ—Ö–æ–¥ –∫ —Å–ª–µ–¥—É—é—â–µ–º—É –¥–Ω—é —Å leftover —Å–ª–æ–≤–∞–º–∏."""
    from utils.helpers import previous_daily_words, reset_daily_words_cache
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º leftover —Å–ª–æ–≤–∞ –≤ previous_daily_words
    if leftover_words:
        previous_daily_words[chat_id] = leftover_words
    elif chat_id in previous_daily_words:
        del previous_daily_words[chat_id]
    
    # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –∫—ç—à, –Ω–æ –Ω–µ previous_daily_words
    reset_daily_words_cache(chat_id)

def get_daily_words_info(chat_id: int, force_new_day: bool = False):
    """–ü–æ–ª—É—á–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–ª–æ–≤–∞—Ö –¥–Ω—è (–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø)."""
    from utils.helpers import get_daily_words_for_user
    from config import REMINDER_START, DURATION_HOURS
    from database import crud
    
    user = crud.get_user(chat_id)
    if not user:
        return None
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º force_reset —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ force_new_day = True
    result = get_daily_words_for_user(
        chat_id, user[1], user[2], user[3],
        first_time=REMINDER_START, duration_hours=DURATION_HOURS,
        force_reset=force_new_day
    )
    
    if result is None:
        return None
        
    messages, times = result
    
    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∫—ç—à–∞
    from utils.helpers import daily_words_cache
    cache_entry = daily_words_cache.get(chat_id)
    
    if cache_entry and len(cache_entry) >= 11:
        is_revision = cache_entry[9]
        unique_words = cache_entry[8]
        prefix = cache_entry[10]
    else:
        is_revision = False
        unique_words = []
        prefix = ""
    
    return {
        "messages": messages,
        "times": times,
        "unique_words": unique_words,
        "is_revision": is_revision,
        "prefix": prefix,
        "word_messages": messages,
        "total_notifications": len(messages)
    }

def test_normal_learning_phase():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –æ–±—ã—á–Ω—É—é —Ñ–∞–∑—É –∏–∑—É—á–µ–Ω–∏—è."""
    print("\nüß™ –¢–ï–°–¢ 1: –û–±—ã—á–Ω–∞—è —Ñ–∞–∑–∞ –∏–∑—É—á–µ–Ω–∏—è")
    print("=" * 50)
    
    chat_id = 999001
    total_words = 45
    words_per_day = 10
    
    try:
        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä
        words = create_test_set("A1", "TestSet", total_words)
        setup_test_user(chat_id, "A1", words_per_day)
        
        print(f"üìö –°–æ–∑–¥–∞–Ω –Ω–∞–±–æ—Ä: {total_words} —Å–ª–æ–≤, –Ω–∞—Å—Ç—Ä–æ–π–∫–∞: {words_per_day} —Å–ª–æ–≤/–¥–µ–Ω—å")
        
        # –î–µ–Ω—å 1: –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å 10 –Ω–æ–≤—ã—Ö —Å–ª–æ–≤
        info = get_daily_words_info(chat_id, force_new_day=True)
        assert info is not None, "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ª–æ–≤–∞ –¥–Ω—è"
        assert len(info["unique_words"]) == words_per_day, f"–û–∂–∏–¥–∞–ª–æ—Å—å {words_per_day} —Å–ª–æ–≤, –ø–æ–ª—É—á–µ–Ω–æ {len(info['unique_words'])}"
        assert not info["is_revision"], "–ù–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —Ä–µ–∂–∏–º–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è"
        assert not info["prefix"], "–ù–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –ø—Ä–µ—Ñ–∏–∫—Å–Ω–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è"
        
        print(f"‚úÖ –î–µ–Ω—å 1: {len(info['unique_words'])} —Å–ª–æ–≤, —Ä–µ–∂–∏–º: –æ–±—ã—á–Ω—ã–π")
        
        # –ò–∑—É—á–∞–µ–º 8 –∏–∑ 10 —Å–ª–æ–≤ (2 –æ—Å—Ç–∞—é—Ç—Å—è –Ω–µ–≤—ã—É—á–µ–Ω–Ω—ã–º–∏)
        words_to_learn = info["unique_words"][:8]
        leftover_words = info["unique_words"][8:]
        simulate_learning_words(chat_id, words_to_learn)
        
        print(f"üìñ –í—ã—É—á–µ–Ω–æ: {len(words_to_learn)} —Å–ª–æ–≤, –æ—Å—Ç–∞–ª–æ—Å—å –Ω–µ–≤—ã—É—á–µ–Ω–Ω—ã–º–∏: {len(leftover_words)}")
        
        # –°–∏–º—É–ª–∏—Ä—É–µ–º –ø–µ—Ä–µ—Ö–æ–¥ –∫ —Å–ª–µ–¥—É—é—â–µ–º—É –¥–Ω—é —Å leftover —Å–ª–æ–≤–∞–º–∏
        simulate_leftover_transition(chat_id, leftover_words)
        
        # –î–µ–Ω—å 2: –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å 2 + 8 = 10 —Å–ª–æ–≤
        info = get_daily_words_info(chat_id, force_new_day=True)
        assert len(info["unique_words"]) == words_per_day, f"–û–∂–∏–¥–∞–ª–æ—Å—å {words_per_day} —Å–ª–æ–≤, –ø–æ–ª—É—á–µ–Ω–æ {len(info['unique_words'])}"
        assert not info["is_revision"], "–ù–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —Ä–µ–∂–∏–º–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ leftover —Å–ª–æ–≤–∞ –≤–∫–ª—é—á–µ–Ω—ã
        leftover_found = sum(1 for word in leftover_words if word in info["unique_words"])
        assert leftover_found == len(leftover_words), f"–ù–µ –≤—Å–µ leftover —Å–ª–æ–≤–∞ –Ω–∞–π–¥–µ–Ω—ã: {leftover_found}/{len(leftover_words)}"
        
        print(f"‚úÖ –î–µ–Ω—å 2: {len(info['unique_words'])} —Å–ª–æ–≤ (–≤–∫–ª—é—á–∞—è {leftover_found} leftover)")
        
        # –ò–∑—É—á–∞–µ–º –≤—Å–µ 10 —Å–ª–æ–≤
        simulate_learning_words(chat_id, info["unique_words"])
        
        print("üìñ –í—ã—É—á–µ–Ω–æ: –≤—Å–µ 10 —Å–ª–æ–≤")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—â–∏–π –ø—Ä–æ–≥—Ä–µ—Å—Å
        from database import crud
        learned = crud.get_learned_words(chat_id)
        print(f"üéØ –û–±—â–∏–π –ø—Ä–æ–≥—Ä–µ—Å—Å: {len(learned)}/{total_words} —Å–ª–æ–≤")
        
        assert len(learned) == 18, f"–û–∂–∏–¥–∞–ª–æ—Å—å 18 –≤—ã—É—á–µ–Ω–Ω—ã—Ö —Å–ª–æ–≤, –ø–æ–ª—É—á–µ–Ω–æ {len(learned)}"
        
        print("‚úÖ –¢–ï–°–¢ 1 –ü–†–û–ô–î–ï–ù: –û–±—ã—á–Ω–∞—è —Ñ–∞–∑–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        return True
        
    except Exception as e:
        print(f"‚ùå –¢–ï–°–¢ 1 –ü–†–û–í–ê–õ–ï–ù: {e}")
        import traceback
        traceback.print_exc()
        return False
    finally:
        cleanup_test_user(chat_id)

def test_remainder_phase():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç —Ñ–∞–∑—É –æ—Å—Ç–∞—Ç–∫–æ–≤."""
    print("\nüß™ –¢–ï–°–¢ 2: –§–∞–∑–∞ –æ—Å—Ç–∞—Ç–∫–æ–≤")
    print("=" * 50)
    
    chat_id = 999002
    total_words = 45
    words_per_day = 10
    
    try:
        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä –∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        words = create_test_set("A1", "TestSet", total_words)
        setup_test_user(chat_id, "A1", words_per_day)
        
        # –°–∏–º—É–ª–∏—Ä—É–µ–º —Å–∏—Ç—É–∞—Ü–∏—é: –≤—ã—É—á–µ–Ω–æ 38 —Å–ª–æ–≤, –æ—Å—Ç–∞–ª–æ—Å—å 7
        words_to_learn = words[:38]
        simulate_learning_words(chat_id, words_to_learn)
        
        print(f"üìö –í—ã—É—á–µ–Ω–æ: {len(words_to_learn)} —Å–ª–æ–≤, –æ—Å—Ç–∞–ª–æ—Å—å: {total_words - len(words_to_learn)}")
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–ª–æ–≤–∞ –¥–Ω—è - –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å 7 —Å–ª–æ–≤ —Å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ–º
        info = get_daily_words_info(chat_id, force_new_day=True)
        remaining_count = total_words - len(words_to_learn)
        
        assert info is not None, "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ª–æ–≤–∞ –¥–Ω—è"
        assert len(info["unique_words"]) == remaining_count, f"–û–∂–∏–¥–∞–ª–æ—Å—å {remaining_count} —Å–ª–æ–≤, –ø–æ–ª—É—á–µ–Ω–æ {len(info['unique_words'])}"
        assert not info["is_revision"], "–ù–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —Ä–µ–∂–∏–º–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è"
        assert info["prefix"].startswith("‚ö†Ô∏è"), "–î–æ–ª–∂–Ω–æ –±—ã—Ç—å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ–± –æ—Å—Ç–∞—Ç–∫–∞—Ö"
        assert str(remaining_count) in info["prefix"], "–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ"
        
        print(f"‚úÖ –§–∞–∑–∞ –æ—Å—Ç–∞—Ç–∫–æ–≤: {len(info['unique_words'])} —Å–ª–æ–≤")
        print(f"‚úÖ –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: {info['prefix'][:50]}...")
        
        # –ò–∑—É—á–∞–µ–º 5 –∏–∑ 7 –æ—Å—Ç–∞—Ç–∫–æ–≤
        words_to_learn_now = info["unique_words"][:5]
        leftover_words = info["unique_words"][5:]
        simulate_learning_words(chat_id, words_to_learn_now)
        
        print(f"üìñ –í—ã—É—á–µ–Ω–æ –µ—â–µ: {len(words_to_learn_now)} —Å–ª–æ–≤")
        
        # –°–∏–º—É–ª–∏—Ä—É–µ–º –ø–µ—Ä–µ—Ö–æ–¥ –∫ —Å–ª–µ–¥—É—é—â–µ–º—É –¥–Ω—é —Å leftover —Å–ª–æ–≤–∞–º–∏
        simulate_leftover_transition(chat_id, leftover_words)
        
        # –°–ª–µ–¥—É—é—â–∏–π –¥–µ–Ω—å: –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å 2 —Å–ª–æ–≤–∞ —Å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ–º
        info = get_daily_words_info(chat_id, force_new_day=True)
        assert len(info["unique_words"]) == 2, f"–û–∂–∏–¥–∞–ª–æ—Å—å 2 —Å–ª–æ–≤–∞, –ø–æ–ª—É—á–µ–Ω–æ {len(info['unique_words'])}"
        assert info["prefix"].startswith("‚ö†Ô∏è"), "–î–æ–ª–∂–Ω–æ –±—ã—Ç—å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ–± –æ—Å—Ç–∞—Ç–∫–∞—Ö"
        assert "2" in info["prefix"], "–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ"
        
        print(f"‚úÖ –°–ª–µ–¥—É—é—â–∏–π –¥–µ–Ω—å: {len(info['unique_words'])} —Å–ª–æ–≤–∞")
        
        print("‚úÖ –¢–ï–°–¢ 2 –ü–†–û–ô–î–ï–ù: –§–∞–∑–∞ –æ—Å—Ç–∞—Ç–∫–æ–≤ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        return True
        
    except Exception as e:
        print(f"‚ùå –¢–ï–°–¢ 2 –ü–†–û–í–ê–õ–ï–ù: {e}")
        import traceback
        traceback.print_exc()
        return False
    finally:
        cleanup_test_user(chat_id)

def test_revision_phase():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç —Ä–µ–∂–∏–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è."""
    print("\nüß™ –¢–ï–°–¢ 3: –†–µ–∂–∏–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è")
    print("=" * 50)
    
    chat_id = 999003
    total_words = 45
    words_per_day = 10
    
    try:
        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä –∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        words = create_test_set("A1", "TestSet", total_words)
        setup_test_user(chat_id, "A1", words_per_day)
        
        # –ò–∑—É—á–∞–µ–º –í–°–ï —Å–ª–æ–≤–∞
        simulate_learning_words(chat_id, words)
        
        print(f"üìö –í—ã—É—á–µ–Ω–æ: –í–°–ï {len(words)} —Å–ª–æ–≤")
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–ª–æ–≤–∞ –¥–Ω—è - –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ä–µ–∂–∏–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è
        info = get_daily_words_info(chat_id, force_new_day=True)
        
        assert info is not None, "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ª–æ–≤–∞ –¥–Ω—è"
        assert info["is_revision"], "–î–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ä–µ–∂–∏–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è"
        assert len(info["unique_words"]) == words_per_day, f"–í —Ä–µ–∂–∏–º–µ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å {words_per_day} —Å–ª–æ–≤"
        assert info["prefix"].startswith("üéì"), "–î–æ–ª–∂–Ω–æ –±—ã—Ç—å –ø–æ–∑–¥—Ä–∞–≤–ª–µ–Ω–∏–µ"
        assert "–≤—ã—É—á–∏–ª–∏ –≤—Å–µ —Å–ª–æ–≤–∞" in info["prefix"], "–°–æ–æ–±—â–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç—å –ø–æ–∑–¥—Ä–∞–≤–ª–µ–Ω–∏–µ"
        
        print(f"‚úÖ –†–µ–∂–∏–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è: {len(info['unique_words'])} —Å–ª–æ–≤")
        print(f"‚úÖ –ü–æ–∑–¥—Ä–∞–≤–ª–µ–Ω–∏–µ: {info['prefix'][:50]}...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å–ª–æ–≤–∞ —Å–ª—É—á–∞–π–Ω—ã–µ –∏–∑ –≤—Å–µ–≥–æ –Ω–∞–±–æ—Ä–∞
        revision_words = set(info["unique_words"])
        all_words = set(words)
        assert revision_words.issubset(all_words), "–°–ª–æ–≤–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞"
        
        print("‚úÖ –°–ª–æ–≤–∞ –¥–ª—è –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –≤—ã–±—Ä–∞–Ω—ã –∏–∑ –≤—Å–µ–≥–æ –Ω–∞–±–æ—Ä–∞")
        
        # –°–ª–µ–¥—É—é—â–∏–π –¥–µ–Ω—å: —Å–Ω–æ–≤–∞ —Ä–µ–∂–∏–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è
        info2 = get_daily_words_info(chat_id, force_new_day=True)
        assert info2["is_revision"], "–†–µ–∂–∏–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è –¥–æ–ª–∂–µ–Ω —Å–æ—Ö—Ä–∞–Ω—è—Ç—å—Å—è"
        assert len(info2["unique_words"]) == words_per_day, "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –¥–æ–ª–∂–Ω–æ –æ—Å—Ç–∞–≤–∞—Ç—å—Å—è –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º"
        
        print("‚úÖ –†–µ–∂–∏–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –º–µ–∂–¥—É –¥–Ω—è–º–∏")
        
        print("‚úÖ –¢–ï–°–¢ 3 –ü–†–û–ô–î–ï–ù: –†–µ–∂–∏–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        return True
        
    except Exception as e:
        print(f"‚ùå –¢–ï–°–¢ 3 –ü–†–û–í–ê–õ–ï–ù: {e}")
        import traceback
        traceback.print_exc()
        return False
    finally:
        cleanup_test_user(chat_id)

def test_leftover_words():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫—É –Ω–µ–≤—ã—É—á–µ–Ω–Ω—ã—Ö —Å–ª–æ–≤ (leftover)."""
    print("\nüß™ –¢–ï–°–¢ 4: –û–±—Ä–∞–±–æ—Ç–∫–∞ leftover —Å–ª–æ–≤")
    print("=" * 50)
    
    chat_id = 999004
    total_words = 20
    words_per_day = 5
    
    try:
        # –°–æ–∑–¥–∞–µ–º –Ω–µ–±–æ–ª—å—à–æ–π –Ω–∞–±–æ—Ä –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        words = create_test_set("A1", "TestSet", total_words)
        setup_test_user(chat_id, "A1", words_per_day)
        
        print(f"üìö –°–æ–∑–¥–∞–Ω –Ω–∞–±–æ—Ä: {total_words} —Å–ª–æ–≤, –Ω–∞—Å—Ç—Ä–æ–π–∫–∞: {words_per_day} —Å–ª–æ–≤/–¥–µ–Ω—å")
        
        # –î–µ–Ω—å 1: –ø–æ–ª—É—á–∞–µ–º 5 —Å–ª–æ–≤
        info = get_daily_words_info(chat_id, force_new_day=True)
        day1_words = info["unique_words"].copy()
        
        # –ò–∑—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ 3 –∏–∑ 5 —Å–ª–æ–≤ (2 –æ—Å—Ç–∞—é—Ç—Å—è leftover)
        learned_words = day1_words[:3]
        leftover_words = day1_words[3:]
        simulate_learning_words(chat_id, learned_words)
        
        print(f"üìñ –î–µ–Ω—å 1: –≤—ã—É—á–µ–Ω–æ {len(learned_words)}, leftover: {len(leftover_words)}")
        
        # –°–∏–º—É–ª–∏—Ä—É–µ–º –ø–µ—Ä–µ—Ö–æ–¥ –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π –¥–µ–Ω—å —Å leftover —Å–ª–æ–≤–∞–º–∏
        simulate_leftover_transition(chat_id, leftover_words)
        
        # –î–µ–Ω—å 2: –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å 2 leftover + 3 –Ω–æ–≤—ã—Ö = 5 —Å–ª–æ–≤
        info = get_daily_words_info(chat_id, force_new_day=True)
        day2_words = info["unique_words"]
        
        assert len(day2_words) == words_per_day, f"–û–∂–∏–¥–∞–ª–æ—Å—å {words_per_day} —Å–ª–æ–≤, –ø–æ–ª—É—á–µ–Ω–æ {len(day2_words)}"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ leftover —Å–ª–æ–≤–∞ –≤–∫–ª—é—á–µ–Ω—ã
        leftover_found = 0
        for leftover in leftover_words:
            if leftover in day2_words:
                leftover_found += 1
        
        assert leftover_found == len(leftover_words), f"–ù–µ –≤—Å–µ leftover —Å–ª–æ–≤–∞ –Ω–∞–π–¥–µ–Ω—ã: {leftover_found}/{len(leftover_words)}"
        
        print(f"‚úÖ –î–µ–Ω—å 2: {len(day2_words)} —Å–ª–æ–≤, –≤–∫–ª—é—á–∞—è {leftover_found} leftover")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–æ–≤—ã–µ —Å–ª–æ–≤–∞
        new_words = [w for w in day2_words if w not in leftover_words]
        expected_new = words_per_day - len(leftover_words)
        assert len(new_words) == expected_new, f"–û–∂–∏–¥–∞–ª–æ—Å—å {expected_new} –Ω–æ–≤—ã—Ö —Å–ª–æ–≤, –ø–æ–ª—É—á–µ–Ω–æ {len(new_words)}"
        
        print(f"‚úÖ –ù–æ–≤—ã—Ö —Å–ª–æ–≤: {len(new_words)}")
        
        print("‚úÖ –¢–ï–°–¢ 4 –ü–†–û–ô–î–ï–ù: Leftover —Å–ª–æ–≤–∞ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        return True
        
    except Exception as e:
        print(f"‚ùå –¢–ï–°–¢ 4 –ü–†–û–í–ê–õ–ï–ù: {e}")
        import traceback
        traceback.print_exc()
        return False
    finally:
        cleanup_test_user(chat_id)

def test_quiz_behavior():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –ø–æ–≤–µ–¥–µ–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤ –≤ —Ä–∞–∑–Ω—ã—Ö —Ä–µ–∂–∏–º–∞—Ö."""
    print("\nüß™ –¢–ï–°–¢ 5: –ü–æ–≤–µ–¥–µ–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤")
    print("=" * 50)
    
    chat_id = 999005
    total_words = 10
    words_per_day = 5
    
    try:
        # –°–æ–∑–¥–∞–µ–º –Ω–µ–±–æ–ª—å—à–æ–π –Ω–∞–±–æ—Ä
        words = create_test_set("A1", "TestSet", total_words)
        setup_test_user(chat_id, "A1", words_per_day)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º —Ç–µ—Å—Ç–∞
        info = get_daily_words_info(chat_id, force_new_day=True)
        
        from utils.helpers import daily_words_cache
        from utils.visual_helpers import extract_english
        from database import crud
        
        # –°–∏–º—É–ª–∏—Ä—É–µ–º –∑–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∞
        learned = {extract_english(w[0]).lower() for w in crud.get_learned_words(chat_id)}
        
        entry = daily_words_cache.get(chat_id)
        unique_words = entry[8] if entry and len(entry) > 8 else []
        revision = bool(len(entry) > 9 and entry[9]) if entry else False
        
        if revision:
            source = unique_words
        else:
            source = [w for w in unique_words if extract_english(w).lower() not in learned]
        
        print(f"üìù –û–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º: {len(source)} —Å–ª–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∞, revision={revision}")
        assert len(source) == words_per_day, "–í –æ–±—ã—á–Ω–æ–º —Ä–µ–∂–∏–º–µ –¥–æ–ª–∂–Ω—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –≤—Å–µ —Å–ª–æ–≤–∞ –¥–Ω—è"
        assert not revision, "–í –æ–±—ã—á–Ω–æ–º —Ä–µ–∂–∏–º–µ –Ω–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —Ñ–ª–∞–≥–∞ revision"
        
        # –ò–∑—É—á–∞–µ–º –≤—Å–µ —Å–ª–æ–≤–∞
        simulate_learning_words(chat_id, words)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∂–∏–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è
        info = get_daily_words_info(chat_id, force_new_day=True)
        entry = daily_words_cache.get(chat_id)
        unique_words = entry[8] if entry and len(entry) > 8 else []
        revision = bool(len(entry) > 9 and entry[9]) if entry else False
        
        learned = {extract_english(w[0]).lower() for w in crud.get_learned_words(chat_id)}
        
        if revision:
            source = unique_words
        else:
            source = [w for w in unique_words if extract_english(w).lower() not in learned]
        
        print(f"üìù –†–µ–∂–∏–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è: {len(source)} —Å–ª–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∞, revision={revision}")
        assert revision, "–í —Ä–µ–∂–∏–º–µ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ñ–ª–∞–≥ revision"
        assert len(source) == words_per_day, "–í —Ä–µ–∂–∏–º–µ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤"
        
        print("‚úÖ –¢–ï–°–¢ 5 –ü–†–û–ô–î–ï–ù: –¢–µ—Å—Ç—ã —Ä–∞–±–æ—Ç–∞—é—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –≤ —Ä–∞–∑–Ω—ã—Ö —Ä–µ–∂–∏–º–∞—Ö")
        return True
        
    except Exception as e:
        print(f"‚ùå –¢–ï–°–¢ 5 –ü–†–û–í–ê–õ–ï–ù: {e}")
        import traceback
        traceback.print_exc()
        return False
    finally:
        cleanup_test_user(chat_id)

def test_edge_cases():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –≥—Ä–∞–Ω–∏—á–Ω—ã–µ —Å–ª—É—á–∞–∏."""
    print("\nüß™ –¢–ï–°–¢ 6: –ì—Ä–∞–Ω–∏—á–Ω—ã–µ —Å–ª—É—á–∞–∏")
    print("=" * 50)
    
    try:
        # –°–ª—É—á–∞–π 1: –ù–∞–±–æ—Ä –º–µ–Ω—å—à–µ —á–µ–º words_per_day
        print("\nüî∏ –°–ª—É—á–∞–π 1: –ù–∞–±–æ—Ä –º–µ–Ω—å—à–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞")
        chat_id = 999006
        total_words = 3
        words_per_day = 10
        
        words = create_test_set("A1", "TestSet", total_words)
        setup_test_user(chat_id, "A1", words_per_day)
        
        info = get_daily_words_info(chat_id, force_new_day=True)
        assert len(info["unique_words"]) == total_words, f"–î–æ–ª–∂–Ω–æ –±—ã—Ç—å {total_words} —Å–ª–æ–≤, –ø–æ–ª—É—á–µ–Ω–æ {len(info['unique_words'])}"
        assert info["prefix"].startswith("‚ö†Ô∏è"), "–î–æ–ª–∂–Ω–æ –±—ã—Ç—å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ"
        assert str(total_words) in info["prefix"], "–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ"
        
        print(f"‚úÖ –°–ª—É—á–∞–π 1: {len(info['unique_words'])}/{total_words} —Å–ª–æ–≤ —Å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ–º")
        cleanup_test_user(chat_id)
        
        # –°–ª—É—á–∞–π 2: words_per_day = 1
        print("\nüî∏ –°–ª—É—á–∞–π 2: –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ –¥–µ–Ω—å")
        chat_id = 999007
        words_per_day = 1
        
        words = create_test_set("A1", "TestSet", 10)
        setup_test_user(chat_id, "A1", words_per_day)
        
        info = get_daily_words_info(chat_id, force_new_day=True)
        assert len(info["unique_words"]) == words_per_day, f"–î–æ–ª–∂–Ω–æ –±—ã—Ç—å {words_per_day} —Å–ª–æ–≤–æ"
        
        print(f"‚úÖ –°–ª—É—á–∞–π 2: {len(info['unique_words'])} —Å–ª–æ–≤–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        cleanup_test_user(chat_id)
        
        # –°–ª—É—á–∞–π 3: –í—Å–µ —Å–ª–æ–≤–∞ leftover
        print("\nüî∏ –°–ª—É—á–∞–π 3: –í—Å–µ —Å–ª–æ–≤–∞ leftover")
        chat_id = 999008
        words_per_day = 5
        
        words = create_test_set("A1", "TestSet", 20)
        setup_test_user(chat_id, "A1", words_per_day)
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–µ—Ä–≤—ã–π –¥–µ–Ω—å –∏ –¥–µ–ª–∞–µ–º –≤—Å–µ —Å–ª–æ–≤–∞ leftover
        info = get_daily_words_info(chat_id, force_new_day=True)
        simulate_leftover_transition(chat_id, info["unique_words"])
        
        info2 = get_daily_words_info(chat_id, force_new_day=True)
        assert len(info2["unique_words"]) == words_per_day, "–î–æ–ª–∂–Ω–æ –±—ã—Ç—å –ø–æ–ª–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ —Ç–µ –∂–µ —Å–ª–æ–≤–∞
        assert set(info2["unique_words"]) == set(info["unique_words"]), "–î–æ–ª–∂–Ω—ã –±—ã—Ç—å —Ç–µ –∂–µ leftover —Å–ª–æ–≤–∞"
        
        print(f"‚úÖ –°–ª—É—á–∞–π 3: {len(info2['unique_words'])} leftover —Å–ª–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        cleanup_test_user(chat_id)
        
        print("‚úÖ –¢–ï–°–¢ 6 –ü–†–û–ô–î–ï–ù: –í—Å–µ –≥—Ä–∞–Ω–∏—á–Ω—ã–µ —Å–ª—É—á–∞–∏ —Ä–∞–±–æ—Ç–∞—é—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        return True
        
    except Exception as e:
        print(f"‚ùå –¢–ï–°–¢ 6 –ü–†–û–í–ê–õ–ï–ù: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_transition_phases():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –ø–µ—Ä–µ—Ö–æ–¥—ã –º–µ–∂–¥—É —Ñ–∞–∑–∞–º–∏."""
    print("\nüß™ –¢–ï–°–¢ 7: –ü–µ—Ä–µ—Ö–æ–¥—ã –º–µ–∂–¥—É —Ñ–∞–∑–∞–º–∏")
    print("=" * 50)
    
    chat_id = 999009
    total_words = 15
    words_per_day = 7
    
    try:
        # –°–æ–∑–¥–∞–µ–º –Ω–∞–±–æ—Ä
        words = create_test_set("A1", "TestSet", total_words)
        setup_test_user(chat_id, "A1", words_per_day)
        
        # –§–∞–∑–∞ 1: –û–±—ã—á–Ω–æ–µ –∏–∑—É—á–µ–Ω–∏–µ (–¥–µ–Ω—å 1)
        print("\nüìÖ –î–µ–Ω—å 1: –û–±—ã—á–Ω–∞—è —Ñ–∞–∑–∞")
        info = get_daily_words_info(chat_id, force_new_day=True)
        assert len(info["unique_words"]) == words_per_day, "–î–æ–ª–∂–Ω–æ –±—ã—Ç—å –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤"
        assert not info["is_revision"], "–ù–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —Ä–µ–∂–∏–º–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è"
        assert not info["prefix"], "–ù–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –ø—Ä–µ—Ñ–∏–∫—Å–∞"
        
        # –ò–∑—É—á–∞–µ–º –≤—Å–µ —Å–ª–æ–≤–∞ –¥–Ω—è 1
        simulate_learning_words(chat_id, info["unique_words"])
        print(f"‚úÖ –î–µ–Ω—å 1: {len(info['unique_words'])} —Å–ª–æ–≤ –∏–∑—É—á–µ–Ω–æ")
        
        # –§–∞–∑–∞ 2: –ü–µ—Ä–µ—Ö–æ–¥ –∫ –æ—Å—Ç–∞—Ç–∫–∞–º (–¥–µ–Ω—å 2)
        print("\nüìÖ –î–µ–Ω—å 2: –ü–µ—Ä–µ—Ö–æ–¥ –∫ —Ñ–∞–∑–µ –æ—Å—Ç–∞—Ç–∫–æ–≤")
        info = get_daily_words_info(chat_id, force_new_day=True)
        remaining = total_words - words_per_day  # 15 - 7 = 8
        assert len(info["unique_words"]) == remaining, f"–î–æ–ª–∂–Ω–æ –±—ã—Ç—å {remaining} –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è —Å–ª–æ–≤"
        assert not info["is_revision"], "–ï—â–µ –Ω–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —Ä–µ–∂–∏–º–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è"
        assert info["prefix"].startswith("‚ö†Ô∏è"), "–î–æ–ª–∂–Ω–æ –±—ã—Ç—å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ–± –æ—Å—Ç–∞—Ç–∫–∞—Ö"
        
        # –ò–∑—É—á–∞–µ–º –≤—Å–µ –æ—Å—Ç–∞–≤—à–∏–µ—Å—è —Å–ª–æ–≤–∞
        simulate_learning_words(chat_id, info["unique_words"])
        print(f"‚úÖ –î–µ–Ω—å 2: {len(info['unique_words'])} –æ—Å—Ç–∞—Ç–∫–æ–≤ –∏–∑—É—á–µ–Ω–æ")
        
        # –§–∞–∑–∞ 3: –ü–µ—Ä–µ—Ö–æ–¥ –∫ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—é (–¥–µ–Ω—å 3)
        print("\nüìÖ –î–µ–Ω—å 3: –ü–µ—Ä–µ—Ö–æ–¥ –∫ —Ä–µ–∂–∏–º—É –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è")
        info = get_daily_words_info(chat_id, force_new_day=True)
        assert len(info["unique_words"]) == words_per_day, "–î–æ–ª–∂–Ω–æ –±—ã—Ç—å –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤"
        assert info["is_revision"], "–î–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ä–µ–∂–∏–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è"
        assert info["prefix"].startswith("üéì"), "–î–æ–ª–∂–Ω–æ –±—ã—Ç—å –ø–æ–∑–¥—Ä–∞–≤–ª–µ–Ω–∏–µ"
        
        print(f"‚úÖ –î–µ–Ω—å 3: {len(info['unique_words'])} —Å–ª–æ–≤ –¥–ª—è –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –ø–µ—Ä–µ—Ö–æ–¥ —Å—Ç–∞–±–∏–ª–µ–Ω
        info2 = get_daily_words_info(chat_id, force_new_day=True)
        assert info2["is_revision"], "–†–µ–∂–∏–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è –¥–æ–ª–∂–µ–Ω —Å–æ—Ö—Ä–∞–Ω—è—Ç—å—Å—è"
        assert len(info2["unique_words"]) == words_per_day, "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —Å—Ç–∞–±–∏–ª—å–Ω—ã–º"
        
        print("‚úÖ –†–µ–∂–∏–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è —Å—Ç–∞–±–∏–ª–µ–Ω")
        
        print("‚úÖ –¢–ï–°–¢ 7 –ü–†–û–ô–î–ï–ù: –ü–µ—Ä–µ—Ö–æ–¥—ã –º–µ–∂–¥—É —Ñ–∞–∑–∞–º–∏ —Ä–∞–±–æ—Ç–∞—é—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        return True
        
    except Exception as e:
        print(f"‚ùå –¢–ï–°–¢ 7 –ü–†–û–í–ê–õ–ï–ù: {e}")
        import traceback
        traceback.print_exc()
        return False
    finally:
        cleanup_test_user(chat_id)

def test_message_formatting():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ —Ä–∞–∑–Ω—ã—Ö —Ä–µ–∂–∏–º–∞—Ö."""
    print("\nüß™ –¢–ï–°–¢ 8: –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏–π")
    print("=" * 50)
    
    chat_id = 999010
    total_words = 12
    words_per_day = 5
    
    try:
        # –°–æ–∑–¥–∞–µ–º –Ω–∞–±–æ—Ä –∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        words = create_test_set("A1", "TestSet", total_words)
        setup_test_user(chat_id, "A1", words_per_day)
        
        from utils.visual_helpers import format_daily_words_message, truncate_daily_words_message
        
        # –¢–µ—Å—Ç 1: –û–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º
        print("\nüî∏ –¢–µ—Å—Ç —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: –û–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º")
        info = get_daily_words_info(chat_id, force_new_day=True)
        formatted = format_daily_words_message(info["messages"], info["times"], "TestSet", total_words)
        
        assert "üìö –°–ª–æ–≤–∞—Ä—å –Ω–∞ —Å–µ–≥–æ–¥–Ω—è" in formatted, "–î–æ–ª–∂–µ–Ω –±—ã—Ç—å –∑–∞–≥–æ–ª–æ–≤–æ–∫"
        assert "TestSet" in formatted, "–î–æ–ª–∂–Ω–æ –±—ã—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–µ –Ω–∞–±–æ—Ä–∞"
        assert f"({total_words} —Å–ª–æ–≤)" in formatted, "–î–æ–ª–∂–Ω–æ –±—ã—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ –Ω–∞–±–æ—Ä–µ"
        assert "‚è∞" in formatted, "–î–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤—Ä–µ–º–µ–Ω–∞"
        
        print("‚úÖ –û–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º: —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        
        # –ò–∑—É—á–∞–µ–º —á–∞—Å—Ç—å —Å–ª–æ–≤ –¥–ª—è –ø–µ—Ä–µ—Ö–æ–¥–∞ –∫ –æ—Å—Ç–∞—Ç–∫–∞–º
        words_to_learn = info["unique_words"][:3]
        leftover_words = info["unique_words"][3:]
        simulate_learning_words(chat_id, words_to_learn)
        simulate_leftover_transition(chat_id, leftover_words)
        
        # –¢–µ—Å—Ç 2: –†–µ–∂–∏–º –æ—Å—Ç–∞—Ç–∫–æ–≤
        print("\nüî∏ –¢–µ—Å—Ç —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: –†–µ–∂–∏–º –æ—Å—Ç–∞—Ç–∫–æ–≤")
        info = get_daily_words_info(chat_id, force_new_day=True)
        formatted = format_daily_words_message(info["messages"], info["times"], "TestSet", total_words)
        
        assert "‚ö†Ô∏è" in formatted, "–î–æ–ª–∂–Ω–æ –±—ã—Ç—å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ"
        assert "–Ω–µ–≤—ã—É—á–µ–Ω–Ω—ã—Ö —Å–ª–æ–≤" in formatted, "–î–æ–ª–∂–Ω–æ –±—ã—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ –æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ"
        
        print("‚úÖ –†–µ–∂–∏–º –æ—Å—Ç–∞—Ç–∫–æ–≤: —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        
        # –ò–∑—É—á–∞–µ–º –≤—Å–µ –æ—Å—Ç–∞–≤—à–∏–µ—Å—è —Å–ª–æ–≤–∞
        simulate_learning_words(chat_id, words)
        
        # –¢–µ—Å—Ç 3: –†–µ–∂–∏–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è
        print("\nüî∏ –¢–µ—Å—Ç —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: –†–µ–∂–∏–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è")
        info = get_daily_words_info(chat_id, force_new_day=True)
        formatted = format_daily_words_message(info["messages"], info["times"], "TestSet", total_words)
        
        assert "üéì" in formatted, "–î–æ–ª–∂–Ω–æ –±—ã—Ç—å –ø–æ–∑–¥—Ä–∞–≤–ª–µ–Ω–∏–µ"
        assert "–≤—ã—É—á–∏–ª–∏ –≤—Å–µ —Å–ª–æ–≤–∞" in formatted, "–î–æ–ª–∂–Ω–æ –±—ã—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ –æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏"
        assert "–ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è" in formatted, "–î–æ–ª–∂–Ω–æ –±—ã—Ç—å —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è"
        
        print("‚úÖ –†–µ–∂–∏–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è: —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        
        # –¢–µ—Å—Ç –æ–±—Ä–µ–∑–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏–π
        print("\nüî∏ –¢–µ—Å—Ç –æ–±—Ä–µ–∑–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π")
        # –°–æ–∑–¥–∞–µ–º –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
        long_message = "üìö –°–ª–æ–≤–∞—Ä—å –Ω–∞ —Å–µ–≥–æ–¥–Ω—è\n" + "‚Ä¢ –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω–æ–µ —Å–ª–æ–≤–æ " * 200
        
        truncated = truncate_daily_words_message(
            long_message, info["unique_words"], words_per_day, 3, "TestSet", total_words
        )
        
        assert len(truncated) < len(long_message), "–°–æ–æ–±—â–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –æ–±—Ä–µ–∑–∞–Ω–æ"
        assert len(truncated) <= 4000, "–û–±—Ä–µ–∑–∞–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –ª–∏–º–∏—Ç–∞"
        
        print("‚úÖ –û–±—Ä–µ–∑–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π: —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        
        print("‚úÖ –¢–ï–°–¢ 8 –ü–†–û–ô–î–ï–ù: –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏–π —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        return True
        
    except Exception as e:
        print(f"‚ùå –¢–ï–°–¢ 8 –ü–†–û–í–ê–õ–ï–ù: {e}")
        import traceback
        traceback.print_exc()
        return False
    finally:
        cleanup_test_user(chat_id)

def test_cache_consistency():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –∫—ç—à–∞."""
    print("\nüß™ –¢–ï–°–¢ 9: –ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –∫—ç—à–∞")
    print("=" * 50)
    
    chat_id = 999011
    total_words = 20
    words_per_day = 8
    
    try:
        # –°–æ–∑–¥–∞–µ–º –Ω–∞–±–æ—Ä –∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        words = create_test_set("A1", "TestSet", total_words)
        setup_test_user(chat_id, "A1", words_per_day)
        
        from utils.helpers import daily_words_cache, reset_daily_words_cache
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–ª–æ–≤–∞ –¥–Ω—è - –¥–æ–ª–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å—Å—è –∫—ç—à
        info1 = get_daily_words_info(chat_id, force_new_day=True)
        assert chat_id in daily_words_cache, "–ö—ç—à –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–æ–∑–¥–∞–Ω"
        
        # –ü–æ–≤—Ç–æ—Ä–Ω—ã–π –∑–∞–ø—Ä–æ—Å –ë–ï–ó force_new_day - –¥–æ–ª–∂–µ–Ω –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫—ç—à
        info2 = get_daily_words_info(chat_id, force_new_day=False)
        assert info1["unique_words"] == info2["unique_words"], "–ö—ç—à –¥–æ–ª–∂–µ–Ω –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å —Ç–µ –∂–µ —Å–ª–æ–≤–∞"
        
        print("‚úÖ –ö—ç—à —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ")
        
        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π —Å–±—Ä–æ—Å –∫—ç—à–∞
        reset_daily_words_cache(chat_id)
        assert chat_id not in daily_words_cache, "–ö—ç—à –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–±—Ä–æ—à–µ–Ω"
        
        # –ù–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –ø–æ—Å–ª–µ —Å–±—Ä–æ—Å–∞
        info3 = get_daily_words_info(chat_id, force_new_day=True)
        assert chat_id in daily_words_cache, "–ö—ç—à –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω"
        
        print("‚úÖ –°–±—Ä–æ—Å –∫—ç—à–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∫—ç—à–∞
        cache_entry = daily_words_cache[chat_id]
        assert len(cache_entry) >= 10, "–ö—ç—à –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–æ–ª—è"
        assert cache_entry[0] == datetime.now().strftime("%Y-%m-%d"), "–î–∞—Ç–∞ –≤ –∫—ç—à–µ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Å–µ–≥–æ–¥–Ω—è—à–Ω–µ–π"
        assert isinstance(cache_entry[8], list), "–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Å–ø–∏—Å–∫–æ–º"
        assert isinstance(cache_entry[9], bool), "–§–ª–∞–≥ revision –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –±—É–ª–µ–≤—ã–º"
        
        print("‚úÖ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫—ç—à–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞")
        
        print("‚úÖ –¢–ï–°–¢ 9 –ü–†–û–ô–î–ï–ù: –ö—ç—à —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ")
        return True
        
    except Exception as e:
        print(f"‚ùå –¢–ï–°–¢ 9 –ü–†–û–í–ê–õ–ï–ù: {e}")
        import traceback
        traceback.print_exc()
        return False
    finally:
        cleanup_test_user(chat_id)

def run_all_tests():
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –≤—Å–µ —Ç–µ—Å—Ç—ã."""
    print("üöÄ –ó–ê–ü–£–°–ö –ü–û–õ–ù–û–ì–û –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø –û–ö–û–ù–ß–ê–ù–ò–Ø –ù–ê–ë–û–†–ê")
    print("=" * 60)
    
    tests = [
        ("–û–±—ã—á–Ω–∞—è —Ñ–∞–∑–∞ –∏–∑—É—á–µ–Ω–∏—è", test_normal_learning_phase),
        ("–§–∞–∑–∞ –æ—Å—Ç–∞—Ç–∫–æ–≤", test_remainder_phase),
        ("–†–µ–∂–∏–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è", test_revision_phase),
        ("–û–±—Ä–∞–±–æ—Ç–∫–∞ leftover —Å–ª–æ–≤", test_leftover_words),
        ("–ü–æ–≤–µ–¥–µ–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤", test_quiz_behavior),
        ("–ì—Ä–∞–Ω–∏—á–Ω—ã–µ —Å–ª—É—á–∞–∏", test_edge_cases),
        ("–ü–µ—Ä–µ—Ö–æ–¥—ã –º–µ–∂–¥—É —Ñ–∞–∑–∞–º–∏", test_transition_phases),
        ("–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏–π", test_message_formatting),
        ("–ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –∫—ç—à–∞", test_cache_consistency),
    ]
    
    passed = 0
    failed = 0
    
    for test_name, test_func in tests:
        print(f"\n{'='*20} {test_name} {'='*20}")
        try:
            if test_func():
                passed += 1
            else:
                failed += 1
        except Exception as e:
            print(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –≤ —Ç–µ—Å—Ç–µ '{test_name}': {e}")
            failed += 1
    
    # –û—á–∏—Å—Ç–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤
    try:
        from pathlib import Path
        from config import LEVELS_DIR
        test_file = Path(LEVELS_DIR) / "A1" / "TestSet.txt"
        if test_file.exists():
            test_file.unlink()
    except Exception as e:
        print(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –Ω–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã: {e}")
    
    print("\n" + "="*60)
    print("üìä –ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´")
    print("="*60)
    print(f"‚úÖ –ü—Ä–æ–π–¥–µ–Ω–æ —Ç–µ—Å—Ç–æ–≤: {passed}")
    print(f"‚ùå –ü—Ä–æ–≤–∞–ª–µ–Ω–æ —Ç–µ—Å—Ç–æ–≤: {failed}")
    print(f"üìà –ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞: {passed/(passed+failed)*100:.1f}%")
    
    if failed == 0:
        print("\nüéâ –í–°–ï –¢–ï–°–¢–´ –ü–†–û–ô–î–ï–ù–´ –£–°–ü–ï–®–ù–û!")
        print("‚úÖ –õ–æ–≥–∏–∫–∞ –æ–∫–æ–Ω—á–∞–Ω–∏—è –Ω–∞–±–æ—Ä–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –≤–æ –≤—Å–µ—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö")
    else:
        print(f"\n‚ö†Ô∏è –ù–ê–ô–î–ï–ù–´ –ü–†–û–ë–õ–ï–ú–´ –í {failed} –¢–ï–°–¢–ê–•")
        print("‚ùå –¢—Ä–µ–±—É–µ—Ç—Å—è –¥–æ—Ä–∞–±–æ—Ç–∫–∞ –ª–æ–≥–∏–∫–∏")
    
    return failed == 0

if __name__ == "__main__":
    success = run_all_tests()
    if success:
        print("\nüéØ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ. –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ!")
    else:
        print("\nüîß –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—ã—è–≤–∏–ª–æ –ø—Ä–æ–±–ª–µ–º—ã. –¢—Ä–µ–±—É–µ—Ç—Å—è –æ—Ç–ª–∞–¥–∫–∞.")